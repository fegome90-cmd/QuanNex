A continuación, se presenta el texto plano completo de la transcripción proporcionada del video "My Top 20 Lessons from Building 100s of AI Agents (Super Actionable)":
i've got an exciting one for you today what you're about to watch is a special live event that I hosted during the launch week of the Dynamus community a ton of value that I've never shared on my YouTube channel before i'm doing this in celebration of the Archon launch and reaching a thousand members in the community so just a thank you to you and a way to show you what we've got going on in Dynamis so this is lessons that I've learned from building hundreds of AI agents over the past couple of years and it was actually really hard to condense all my lessons down into the top 20 that I present here in this event so a ton of value packed into this and so with that let's get right into the event and also let me know in the comments if there's any strategies or tips that I cover here that you want me to make a full video on there we go and so the plan for today is just to give you guys a bunch of really practical lessons that I've learned um just from building hundreds of agents over the last few years here mostly the last couple of years and I typically don't do presentation style videos or live streams i'm usually either just chatting with you guys like I do in live streams or when I'm making a video it's always like really hands-on like let's code something i want this to be like really practical and hands-on um and so even though it is a presentation I'm going to work hard to make it really engaging for you guys so typically I'm against just like showing slides um but yeah I've got my usual mix of like a little bit of memes and just a ton of practical advice for you guys here uh to keep it fun so yeah there's 20 lessons in total that I want to share and uh the first thing I want to say before I kick this off at all is that um it was really hard to take something like lessons learned from building AI agents and like actually um put it down into a presentation that I can give in less than 10 hours i mean there's so many things that I can talk about different lessons that I've learned and so the most difficult part out of all this is just thinking about what are like the few that are the most important that I can share with you guys and so I could probably even do like another event just like this just sharing a whole another set of 20 lessons um but these 20 are the ones that I wanted to focus on the most with you guys and so yeah we'll just go through this right now um and starting with I want to kick this off here by just giving everyone a quick reminder of what an AI agent actually is so I give this definition in the AI agent mastery course and a lot of you already know it at a high level what an agent is but I just want to give this um as a refresher for everyone so an AI agent is a program that uses a large language model to reason about how it interacts with its environment and to take varying courses of actions to achieve a goal and so there's kind of two parts to that first we have interacting with the environment those are the tools that we give the agent to do something like um pull messages from a slack conversation or draft an email in Gmail or Outlook like that's the it interacting with its environment and then taking varying courses of actions the thing that makes agents so powerful and also dangerous as we'll see in a little bit is that it can decide what it wants to do based on what goal you have for it and so it might decide to take five steps or three steps or one we don't know it's it's non-deterministic in that way because we're letting the agent have that level of control and so that's an agent at a high level and then what I want to dive into for my lessons with you guys today is I want to start with u a few highlevel lessons that I've learned from building agents so just kind of zooming out to the top level of just building agents as a whole and then I want to give you lessons that are specific to each of the different components that we have for an AI agent and so if you've been through the first part of the AI agent mastery course you've seen this graphic already because one of the very first videos that I have in module one of the course is breaking down the different components to an AI agent We have the agent program this is the system prompt your general instructions to the agent for how you want it to behave and the tone that you want it to have you have the large language model that's the brain of your agent that has all that reasoning capability and then you have the tools these are the capabilities that you give your agent to interact with its environment and then last we have the memory system so that is both short-term memory like the conversation history and then also long-term memory being able to remember your preferences and goals and things like that between conversations and also rag and just like knowledge bases in general is often considered a part of long-term memory within the memory systems as well so yet another quick refresher there on the components of an AI agent i'm going to give you lessons that I've learned specific to each of these just as a way to keep this presentation really really organized for you guys um and so then after I go through all this I'll I'll also open up a time for a Q&A um probably within the last like 15 20 minutes of the hour that we have here um and so I'm going to try to go through one less lesson every two minutes because I have 20 in total so I'm going to go through things pretty quickly here um and so this is the kind of thing where it'd probably be beneficial to even like watch the replay of this later if you want to pick out certain lessons and really listen in on those again um because I am going to go rather quickly but yeah the goal with this is like even if you're already familiar with half of these lessons and like you already understand that and you implement that for your own agents hopefully at least like five or 10 of these are going to really resonate with you and be useful and like I've got a ton of really practical stuff to share with you guys here so yeah that's an overview of everything and now diving into it starting with the higher level lessons that I've learned a lot of it is based around the biggest challenge that we have with agents and that is the whole idea of hallucinations and I pulled this uh this image off the internet i just thought it was funny uh because it's an AI agent with a long nose like Pinocchio like every time he lies his nose grows bigger and I think this is really uh this really just hits spoton because when AI agents hallucinate as in they make a mistake or they just completely make up information they act so confident when they tell you that information that's just completely wrong like they'll even cite sources from the internet that just don't exist and they'll just tell you with 100% certainty that like this is what it used um for its information and so yeah they they just seem to like lie very freely sometimes um and that's what a hallucination is and I know that in general hallucinations it's a very overused term um and we hear this so much in the AI space but I just think it's a good label for a lot of these mistakes that agents and just LLMs in general can make and then they're also very confident as they present those mistakes to you so that's the biggest challenge that we have and so a lot of the highlevel tips and and lessons that I've learned that I want to share with you guys is working around these hallucinations and so the biggest reason that we have to deal with this in the first place is because for the first time ever when developing software we have the whole idea of non-determinism and that's a a very fancy word um but I like using that a lot essentially what it means is before we had AI agents when we built a piece of software we would take some input like this is someone's email or this is a request that we have to an API endpoint like whatever that input is and then we would always get the exact same output every single time we run that automation or whatever it is with that given input and so it doesn't matter if we run that workflow 100 times if we give the same input we're always going to get the same output that's what I mean by a deterministic workflow and I've covered this in the course as well and so with non-determinism now with an agent when we receive a certain piece of input we aren't guaranteed to always get the same output out because the agent is giving that ability to reason about what it wants to do and it will make those decisions differently even when you give it the same exact prompt and so the danger with this is that it could do fine the first 50 times you give it a certain prompt like you ask it to uh research the web for something specific or whatever that prompt might be but then the 51st time that you give that exact same request it can completely botch it it might search the wrong thing or not even invoke the tool that you want it to invoke even though the last 50 times it did it fine and so that kind of brings me to the first big tip that I have here is that to avoid hallucinations really affecting you deeply you want to use AI to save time not replace you entirely and so I have this uh quote here from uh Peter Parker Spider-Man's uncle he says "With great power comes great responsibility." So these AI agents they're dangerous because they reason about how they want to accomplish a goal for the first time ever we have software that doesn't always behave the same might work in one case and then break in another case with the same input and that's dangerous but also it's powerful having agents being able to reason about what they do in their environment unlocks a whole new level of automations and ways that we can um improve efficiencies in our business there's so many different things we can do with agents uh but the the goal here is to share the responsibility with the AI so that you can catch hallucinations and to make this like really really clear I have a good example here and I I did share this in the AI agent mastery course as well but this is just my favorite example in general i always say that like you wouldn't want to trust an AI agent to manage your inbox and automatically reply to every email that is way too much responsibility that you're giving it you want to share that responsibility and so the way that you can do that is instead of having the agent always reply automatically to the emails it can just automatically create email drafts and so it's still saving you time because it gets you started on each reply um to your emails but then if it hallucinates and it gives a really bad response like maybe it tells the um person who emailed you that you can meet at a time that you're not actually available well then you can correct it and so any mistake any hallucination that it makes is just going to cost you a little bit of time because you have to correct it a bit versus it would actually send that email and um in that case it's replacing you entirely and then that responsibility becomes very dangerous so I hope that makes sense and I don't want to just say that like you never want to build out a process with AI that replaces something you're doing entirely um but the safest bets for implementations for AI um for yourself or your business whatever that is is usually the ones that are going to save you time and then any kind of hallucinations aren't going to like negatively impact you greatly like sending a an email to someone that is just like a totally terrible thing you wouldn't want to actually say so yeah I hope that makes sense that is the the first one there and then the second lesson again just saying really high level at this point is you don't want to skimp on the planning and prototyping phase of building your agents and so I covered this in my road map for building AI agents that's what we're looking at right here for this screenshot and I dedicate two entire modules in the course to this because it's just so important to have a good foundation for your agent you don't want to dive into the implementation of your agent right away without knowing what you're really working towards and what kind of tools you want for your agent and things like that it's worth spending that time up front because if you spend five hours planning your agent that could end up saving you 20 hours of development down the line so it might kind of sting at first like "Oh I'm spending so much time not actually making something yet." but it's going to make the whole process faster in the end you just kind of have to have that foresight and and be patient with the first part of the process and so that's why I cover it so much in in what I do um within the course and just this is how I build my agents in general u because there have definitely been times when I first started building agents that I um I didn't do a lot of planning and prototyping and then I just had to scrap everything and start again and that's when it gets to that point where it's like oh I actually ended up having to spend 20 more hours on this agent um because I didn't spend a few hours planning up front so yeah do not skimp on these phases uh then the the next one here is to beware what I like to call the hallucination explosion or another word for this is uh compounding non-determinism and whoa Cole that those are really fancy phrases all I mean by this though is that if you have many agents that are working together to solve one problem like a multi- aent workflow you have to be careful about the inaccuracies of all these LLMs and they kind of build on top of each other like for example if you have three agents that each individually work well 95% of the time as in they only hallucinate 5% of the time the fact that all three of them have to work correctly for a single execution to be you know quote unquote successful that means that your whole system is only actually going to work 86% of the time and um actually let me pull up a calculator here because I I'll I'll do like a little live calculation to show you how I got this number because let's say agent number one it works successfully 95% of the time so 0 n5 but then it also relies on the second agent like maybe it's communicating to it through the A2A protocol like I covered on my channel previously let's say that this agent also only works successfully 95% of the time well you have to actually multiply these two numbers together if you want to compute how likely your entire system is to be successful but then let's say that it's using the MCP protocol to work with some external tools and maybe it only leverages this MCP server correctly also 95% of the time well then yet again you have to multiply 95% into this equation and so rounding up that's what gives us that 86% that I have in the slide right here so only 86% of the time is this solution actually going to work correctly because even if two of the components here are successful you still might have one that fails and that could bring the entire system down and so that's why you have to be very very careful with kind of what I like to call the hallucination explosion it's an explosion because the more agents that you have in the mix the more likely you are to have these errors kind of compound on each other or I guess I should say the chance of these errors compounding on each other and so the biggest way to tackle this is to just have strategies in general to reduce hallucinations and so this gets into uh the next few lessons that I have here from building AI agents which are all about how to reduce hallucinations in general nowadays when building my AI agents I always rely on AI coding assistants to exponentially speed up my development and that brings us to the sponsor of today's video which is Windinsurf and Windinsurf has a special place in my heart because it was my first ever AI coding assistant seriously before Windsurf I was working in VS Code no extensions like Copilot windsurf has always stood out to me for a few different reasons first of all it's speed like you can watch here on the right hand side in our Cascade agent also we have access to all of the best third-party models listed right here and they've even built their own it's crazy and then also within their Cascade agent there's just so many different features that we have cascade can handle complex coding tasks with longerterm agentic memory we can run multiple conversations in parallel and they've added a new native planning mode so that we can handle even longer more complex tasks with ease the important thing to know is that this is not just a Vive coding tool this is an enterprise ready solution that's actually already being used by over 30% of Fortune 100 companies and with the Cognition acquisition the creators of Devon Windsurf has gotten a makeover it's a lot more reliable and snappy they're continuing to add a ton of new features like Deep Wiki and Vibe and replace and they're not slowing down anytime soon plus I am seriously looking forward to the native Devon and Windsurf integration that's coming soon i'll have a link to Windsurf in the description they are definitely worth checking out yeah one thing that Benedict said I think you can reduce hallucinations a lot if you let another AI check the output and I'm glad you mentioned that because literally that's what I'm covering now in this in this fourth lesson right here is the whole idea of AI agent guard rails and so what guardrails are is just a piece of logic that you have that runs either before you call into your agent or your LLM or after and so then if you detect something that is either going to likely cause hallucinations or you you do detect a hallucination in the output guardrail then you can go down this failure path where you'd either let the user know that their process um or that their request wasn't able to be handled for whatever reason or maybe you'd loop loop back to the AI agent and have it try to correct itself You can take proper course of action based on if one of these guard rails fails and to give you a clear example of what these guardrails look like let's say that you have an agent that is a travel planning agent um and so you as a user you give it the destination your budget how long you want to go on your vacation and then the agent will help you plan an itinerary it's a very common use case that I've seen before a lot of people build this as an example when they show new AI agent frameworks um and so as an input guard rail one thing that you want to avoid for this travel planning assistant is you don't want to have it plan an itinerary um when you are extremely under budget like if you want to go to Dubai for a week for only $500 you probably couldn't even buy the plane ticket for that much depending on where you live and so that would definitely cause a lot of hallucinations if you were forcing the agent to try to uh plan that trip when it's extremely under budget and so a good input guardrail is you could have a smaller more lightweight LLM that just quickly evaluates the user's request for their trip and determines is the budget actually reasonable for the trip and so then if they do say like I want to go to Dubai for $500 then it'll go down this failure path where you would tell the user like hey this is way under budget you've got to adjust something here um for the travel planning assistant to actually be able to help you so that would be a really good guardrail but then if they want to go to Dubai for a weekend maybe it's like $3,000 or whatever like something within a reasonable range then it could continue to the travel planning assistant and then for an output guard rail maybe you would have another agent that is verifying that the travel planning assistant actually planned an itinerary for the number of days that the user said they wanted to go on a vacation for and so if they said that I want to go to Germany for 10 days and the itinerary was only for 8 days well then this LLM would be able to detect that pretty easily like that's the thing with these guardrail agents is a lot of times they can be a lot more lightweight like you could use GPT 4.1 mini instead of GPT 4.1 because a lot of these decisions that it has to make around like is this input or output good they're a lot simpler and so you could just detect really quick like does this itinerary actually make sense given what the user um said and then if it is then you return that file final itinerary otherwise maybe this output guardrail would loop back uh in a failure case and have the agent replan the itinerary so guardrails are super super powerful in general it's just a huge way to reduce hallucinations and a lot of times they can be pretty simple to set up uh and then the next strategy for reducing hallucinations is yet another thing that I've covered a lot before is the whole idea of specialized agents and this one is really a lot simpler to explain um than guard rails because really if you think about it like the way that companies work with humans with people is that you always want to distribute responsibility between many people that specialize in different things you're always going to get better results when you have specialization when you have distributed responsibility among humans and it's the same way it works the same way with agents and so if you have one agent that handles all of your tool calls for Slack and then another one that handles all of your tool calls for working with your database you're distributing that responsibility that's going to lead to a lot better results because if you overwhelm one agent one LLM with way too much responsibility it's going to start to fail frequently understanding how to leverage these different capabilities that you've given it to solve the goal that you have for it and so I've covered this a lot before with like having specialized agents that each use an MCP server things like that but in general this just helps a lot and the one thing you do have to be a little careful of here is this does kind of start to point us back to the hallucination explosion I was just talking about because now you do have an orchestrator agent that has to make the right call like which specialized agent do I point to but typically this decision can be quite simple so you don't have to worry too much about that compounding non-determinism that I was talking about because there at least I've seen from the agents that I've built that this primary orchestrator that has to figure out which specialized agent to use it very rarely picks the wrong one especially if the roles for each of these agents are very distinct and I'm very clear in the system prompt to my primary agent exactly when it needs to call into each of these specialized agents and so this helps reduce hallucinations quite a bit and then the very last strategy that I have here for reducing hallucinations and yeah these are these are all taken up in the lessons as well so this is lesson number six um is examples examples and examples and I'll talk about this more when I get to one of my lessons for system prompting specifically but it is just so helpful to an LLM or AI agents in general to give examples in the system prompt and we see this with all of the the best AI agents and the most popular AI coding assistants like Vzero and Cursor and Bolt new like they all have very concrete examples in their system prompts and so like this screenshot that I took right here this is an example of um it's an example of an example in the system prompt for Bolt the front-end application developer that you can use in your browser um and so yeah like it even says right here like example colon and then they just paste in an example of exactly what they'd want the bolt new agent to output in a specific case and they don't hold back from including any context like they have all the nitty-gritty details of the pluses and the minuses and the at symbols and all the different line numbers for diffs like they've got everything covered in this example here just showing the agent exactly what to output given some input that they describe kind of above here in the system prompt that's cut off right now but yeah examples are just so so powerful and you can use examples to tell an agent how to use tools um exactly the format that you want it to output pretty much any kind of direction that you want to give to the agent examples oftentimes help with that and sometimes they can be overkill like they'll make your system prompt longer um but in general they're really useful especially for anything that's a bit more complex with your agent and so yeah hope that one makes sense there um yeah and then the the next thing that um I want to cover now is diving specifically into lessons for one of the components that we have for an AI agent and that is the agent program and so again this is the system prompt for your agent um the instructions that you give it to determine its behavior and um the tone that you want it to have and so the first lesson that I've got for your system prompt for your agent is that you want to avoid adding what I like to call negatives and this is especially true for longer system prompts and so as a bad example of prompting you might say something like "Explain quantum physics to me and do not use complex language." That's a negative right here like you're telling it not to do something and then a good example of a prompt is explain quantum physics to me and use fifth grade level English and so in this case in selling in instead of telling it not to do something you are telling it to do something and the reason that you want to do this is when you get very long system prompts i've seen this time and time again i don't really know why but LLMs they love to drop the negative as in they will kind of like take out the do not and then they'll do the very thing that you told them not to do like they'll use complex language and obviously in this example here it's a very very basic prompt and and this really only applies to longer system prompts like of course it would listen in this case but once you have system prompts that are like five six 10 paragraphs it will start to do this thing where it'll drop the negative and so you'll see something like this a lot where like when you want it to you know give a very simple explanation for something you'll tell it to like use a specific type of language like high school language or fifth grade level language i'm sure you guys have seen that a lot um yeah Benedict said "This behavior also refers to humans humans also don't hear the negatives." Yeah it's true and and yeah I mean kind of like what I was talking about with specialized agents too like a lot of the ways that we think about how humans behave and how to make humans uh do things well i mean that applies to AI agents as well um so yeah that's a good point um and then the next thing for system prompts and this is probably the most obvious lesson out of all the 20 that I have for you guys here um but this is this happens very frequently still is you want to avoid contradictions in your system prompts and so like in this example that I have this is just a fake system prompt that I had Claude help me generate just for an example for you guys we tell it that it's a knowledge assistant at the top here and we're telling it to give very concise and efficient answers but then in the very next paragraph we tell it to make sure that we give a comprehensive coverage of the topic and we include historical context and theoretical frameworks practical applications blah blah blah blah we're telling it to be like very comprehensive in its response in the very next paragraph and this is a contradiction the LLM is going to take these two pieces of instruction try to balance what it wants to do and it's going to usually just kind of randomly pick between one of these like either be very concise or be super comprehensive and you're going to get very inconsistent results and a lot of hallucinations and I've seen this so much and obviously when you're creating your system prompts you you probably don't want to or you're probably not going to end up making a very obvious contradiction like within two adjacent paragraphs like that probably won't happen but when you have longer system prompts and you're telling it at the start to do something then maybe six paragraphs later you're describing how you want to take a specific action and that's going to contradict with kind of like the role you gave it at the start of the assistant prompt we see this kind of thing a lot and so like one example uh of what I did there was an agent that I built one time that was kind of like a customer support agent and its goal was to get the person communicating with it booked um for an event on the calendar and what I said at the start of the system prompt is I told it to be very flexible and accommodating for when that user is available for getting that that meeting uh booked but then later on in the system prompt when I was giving it instructions on how to use the calendar tools that I had for it I told it to be rigid as in you look at the calendar to see the times that are available for a meeting and you only ever offer those times you never offer times that are outside of the range that you got available times for and so there was a contradiction there i told it to be very rigid when I was describing how to use the tools but then at the start of the system prompt I told it to be very accommodating to the user and so I ended up finding that a lot of times it would offer times to the user that weren't actually available the calendar API did not return to it and it was just because I told it at the start of the system prompt to be accommodating to the user so as soon as I took that out or at least I clarified you want to be accommodating um for um maybe just like I don't know like the like you want to have a nice tone versus like actually be accommodating for the calendar then I stopped getting those hallucinations it started to only ever give times to the user that it got from the calendar API so very important to keep those kind of things in mind so when you start to get like really mixed results where sometimes an agent does things properly and sometimes it doesn't it's usually because of some kind of underlying contradiction that you missed in your system prompt and it happens more than you would think um and then the the last system prompt tip that I have for you guys i'm just checking the time here to make sure that I'm doing good looks pretty good yeah so the next system prompt tip that I have for you guys is to version your prompts and so just like you want to version your code so that you can revert back to a different version of your code if something more recently breaks you want to be able to do the same thing with your system prompts because as you are evolving your system prompts over time even if you think that you have provided more context and of course it would make the AI agent perform better you still will run into situations where maybe your system prompt gets too long and it overwhelms the LLM maybe you added in one of those contradictions and you didn't realize it you'll hit that kind of snag where you want to quickly revert to a previous version of your prompt that maybe wasn't quite working to your liking and that's why you tried another version but at least it was better and so you can revert back to that and then work through your other version back in your development environment and see if there's some contradiction you accidentally added or if you have to give better examples like whatever that is to improve that third version then you can go back to it so versioning is very important just like it is for your code so uh yeah and Jiren asked "What tools are you using to manage your prompts?" Langfuse is a really good one for sure um and then also just like having a separate place in your GitHub repository for your prompts and just versioning it along with your code that definitely works as well um but tools like Langfuse are also fantastic um so yeah that's everything for the system prompt now we move on to large language models and the general lessons that I learned from working with LLMs within AI agents um the first tip that I got for this is that swapping large language models can actually be pretty dangerous even if you are swapping to an LLM that is supposed to be better in just every way um like just as an example a couple of years ago or I guess whenever GPT40 was released I had an agent that was running with GPT4 Turbo that's kind of like the last LLM that OpenAI had released before they released GPT40 and they they gave the benchmarks when they released 40 and they're like "Yep this is just better than GPT4 Turbo in every way uh we recommend that you guys upgrade to this model." And so I did that i took my agent and I just flipped the switch and I changed the LLM from from 4 Turbo to 40 and things were better in some ways but then it started having these like super weird hallucinations because different LLMs even if it's like supposed to be just a straightup better LLM they understand system prompts in different ways and they take actions in different ways and so you have to be very careful for that you can't just swap your LLM push your agent to production with that change and then just call it good like you have to do a lot of testing under the hood to make sure that your system prompt still holds for that new LM that you're using because like I said even if the LLM is better that doesn't actually mean that your agent is going to perform better you might have to tweak your system prompt you might have to um even change your tools that you have for your agent i mean hopefully you wouldn't have to do that usually it's just changing the system prompt that still has to happen a lot and we've seen that before with other things like bolt.diy as well um and like I think Bolt new had this even happen as well where like they were originally running with cloud 3.5 sonnet and then when they changed to cloud 3.7 sonnet they had to tweak their system prompt a ton um like actually more than you would think and so yeah it's really good to just keep that in mind in general and then also your favorite LLM isn't always the best and I see this happen a lot i've been prone to this myself even where I get super attached to a specific LLM and I don't really consider using other ones even though I know deep down that there are different LLMs that are better at different things like sometimes you might get really attached to Claude 3.7 Sonnet like that's just your favorite LLM and you try to use it all of the time and I find Claude 3.7 Sonnet um to generally be the best LLM for coding um overall i mean there's a lot of debate for that but then Gemini 2.5 Pro I find works better for more of my creative agents like they need kind of like writing assistant or something like that and so just knowing those differences of when different LLMs work better and not just being super stubborn and always sticking with one like that is super important like maybe you love using the Quen models like the new Quen 3 that was just released um but you might find with a little bit of testing and if you allow yourself to explore that once in a while with a little bit of testing that mistrol actually works better for one or two your agents for example and so then you can switch to that for your local AI implementation so yeah it's just important to be open-minded especially as more and more LM are released being willing to like test those and not just like look at the benchmarks and try to make a snap decision based off that and it's easy to swap LLMs and test things really quickly so it's worth doing so and then the last lesson that I have from LLM specifically is that you want to watch your context links and this applies mostly to local LLMs um because especially like a lot of the big guys now like Gemini 2.5 Pro or Llama 4 Claude like they all have such long context lengths that you usually don't hit that limit anymore but there still are a lot of LLMs where you do um like a lot of OpenAI's models are 128,000 tokens for their contacts length like you can definitely hit that if you um are retrieving a bunch of chunks from rag or pasting in big documents and then local LLMs oftentimes have a lot smaller contacts limits like 32,000 tokens and you'll often hit those because the biggest thing is that the conversation history is a part of the prompt to the LLM and so as you're sending in more messages and having a longer conversation you can hit this limit and the problem is once you hit that limit you start to lose part of the conversation it's it's included in this like forgotten context at the end here because the limit only spans this part of the conversation and the very important thing to keep in mind with this is that the system prompt like your general instructions to the agent that we've already covered that's generally included at the very top of a conversation because it's the highlevel instructions and so when you hit that context length you'll see that the system prompt is the first thing to go You start to lose those instructions for the agent and so you'll see this a lot with local LMS especially when the agent starts to seem to just completely forget your instructions and not understand how to use the tools anymore that's generally because you hit the context length and now you're starting to lose your system prompt so it doesn't even know it has access to the web search tool anymore so if it's not calling in a brave anymore and you have no idea why you're in a longer conversation it's definitely because of a context limit issue so certainly good to keep that in mind um and so yeah that brings us into the memory systems here long-term and short-term memory so I've got a few lessons for this as well the first one for short-term memory is that previous hallucinations are likely to be repeated by the agent and I've got another really simple example here that I just had Claude generate for me and usually it's not going to be this small of a conversation for this issue to crop up but it just demonstrates it well so like in this case the user is asking uh when was to kill Kill a Mockingbird published and then the AI said it was published in 1962 and then you as the user you'll correct it and you'll say no actually it was published in 1960 and then you guys have all probably seen this before when you correct the LLM it'll say something like "You're absolutely right i apologize for the error yes it was indeed published in 1960." And so then now you would think that it would have that correct information but then if you continue the conversation a while and you come back and say something like "Tell me about Harper Lee and her famous novel." It might repeat that hallucination that it was published in 1962 and you'll see this a lot where even though you corrected its mistake it'll still repeat it in the same conversation and that's one of the reasons why it's really dangerous to have long conversations especially in things like your AI coding assistance because if it understood something incorrectly in your codebase or in the conversation previously it might make that same mistake again i've seen this before where like there was an error invoking a tool like the web search tool and then later on the conversation you ask it to do the web search again and it'll say that it can't like it won't even try because it just knows from the previous point of the conversation that that didn't work even if you told it later that oh yeah you just did this thing wrong that's why it didn't work so yeah that's one big reason why you want to start new conversations quite often with your agents and AI coding assistants and the next thing here I've got the uh prototype pulled up for the N8N agent that we build in the AI agent mastery course because the biggest thing that helped me understand long-term memory better and like maybe this is obvious for a lot of you guys but this really hit home for me is that long-term memory for your agents is just another rag like that's literally all it is so within our prototype here we have the tool for our agent to retrieve memories and then we have the tool for it to retrieve documents both of these just using rag like you can see that the tool is exactly the same and then the way that we insert memories into supabbase is using the exact same node in n as how we insert our documents into superbase within our rag pipeline and the reason this is so important is because a lot of these strategies that we learn for better rag retrieval like query expansion or re-ranking like all of these nitty-gritty strategies which I'm going to be covering in the live workshop this Friday by the way so go to the events tab and check that out if you're interested i'm going to be diving into rag strategies but a lot of those rag strategies apply to working with long-term memory as well because it just is essentially another table in your vector database just with memories generated by the agent instead of documents that you're pulling from your data source like Google Drive your local files SharePoint whatever that might be and obviously not all of the strategies will apply to memories and like you have to treat them differently than your documents um but in general a lot of those ways to make rag more accurate are going to apply to memories as well so really good to know that um and then the last memory lesson that I have here is that you want to include tool calls in your conversation history and I I got this nice diagram actually from a lang chain documentation page that shows what I mean by this so typically what I've I've seen people do this a lot and actually it's really unfortunate the NAND agents do this as well where they don't include in the conversation history anything related to tools and so you store the human message then the agent response and then you just flip back and forth between these and that makes up your entire conversation but really what we want to do is we want to include everything related to the tools as well so whenever the agent makes a request to use a tool like send an email or read a Slack message we want to include that request and then the response that we get back from calling this tool like here is the message or here's the action and the success from that like we want to store that in the conversation history and the reason for that is because a lot of times the agent can re-reference things that were returned by the tool calls in a subsequent request that we have for it in the conversation as a good example of this uh we have rag like when you you perform a search in your vector database and you retrieve relevant chunks the agent is going to use that to then answer the user's message but then the user might have a follow-up message that could also be answered from the chunks that it retrieved from that first rag lookup and so if we have the tool responses as a part of the conversation history the agent can re-reference that to answer the second user question without having to do another lookup so it's just faster and it just makes the agent more likely to give a correct response because it's referencing things that it already retrieved successfully and so we don't have that at all if we just don't in if we don't store the tool calls and responses in the conversation very very very important and then the very last one that I want to cover here is and I'm going to give maybe like seven minutes to try to get through this pretty quickly here because I want to do some time with uh for Q&A with you guys i want to focus on the tools briefly here and really I want to just dive into the anatomy of a good tool how you can set up a tool all the different components and I want this to apply no matter the tool or framework that you're using so if you're using N8N or Pantic AI Crew AI Lang Chain whatever it is like everything that I'm talking about for tools here are going to apply and so the first thing is the descriptions that you give to your tools are key because everything that you give as a part of the tool description is indeed included in the prompt to the LLM to tell it when and how to use a tool and so it's worth being pretty descriptive here and so telling the agent like what's the purpose of this tool how do you call it what are the arguments that are here like for this rag tool we have to pass in some kind of query so that we can search the vector database with that query and my general rule of thumb when working with tools in general is that the tool description like what we're looking at right here that is your place to tell the agent how to use the tool this individual tool and then the system prompt is where you can tell the agent how to use different tools together and so that is where you kind of zoom out at a higher level how do you incorporate these tools together to accomplish a single goal and then the tool description is where you hone in on all of the nitty-gritty details for an individual tool and both are very important because a lot of times you don't want to just use one tool per execution like you want your agents to reason about how they can use different tools together and so that's why you want to include that in the system prompt as well and then the next one is for more complex tools especially just like we do in our system prompts we want to give examples specifically examples for what the parameters might look like so like for this rag this retrieval tool for the user query we're giving it some examples of how we want to format the queries that we would use to search the vector database and so like I said for very simple tools or especially ones that don't have any parameters at all like if we just want to fetch the channels that are available to us in Slack for example we might not need examples but oftentimes they can be very helpful especially when our parameters might be a little bit more complex or we're very particular about how we wanted to format things like our queries and then also for pretty much all of your tools you want to catch the errors and then return the problem to the agent and so you'll see this in my template that I have for module 4 of the pyantic a the pantic AI agent for um the AI agent mastery course every single tool that I create for the agent I always wrap the entire thing in a try block and then I have an accept here and you can do something similar in in N8N with um the like error workflows and for any other framework like this i'm showing a tool from my Pantic AI agent but you can set up something pretty much the exact same in any framework as well like Crew AI or Eggno or whatever that is you just catch any exception here and the reason you want to do this is there's twofold the first is that if an agent uses a tool incorrectly you don't want your application to crash sometimes a tool is going to fail just because the agent hallucinated a bad parameter it's not because your code is actually bad and so you don't want to just like you know crash the entire application the other thing is that when you tell the agent exactly what went wrong and I don't really do that here this isn't a best example cuz I just returned an empty list but if you were to maybe return an empty list and some error message as well you can tell the agent what went wrong with the tool call so that it has a chance to maybe invoke the tool a second time fixing up the parameters or whatever might have caused that error and so that kind of just gives it another level of reasoning like not only what tools should I call on how but also how should I reinvoke them if there are any issues that come up it's very powerful and then the next thing is for your tools you want to be very careful to only return what the LLM needs to know and this actually came up in in our community yesterday someone was talking about using the um Shopify API mark was talking about using the Shopify API in one of his agents and for the tool calls that invoke the Shopify API the results that you get back like this is what we're doing at Superbase we're fetching a bunch of records from Superbase and we're storing that in result here but often times when we're using APIs from different services like Superbase or Shopify what we have in results is going to include a lot of metadata like maybe the time for this query or other pieces of information that we need related to the query that isn't like just the data that we care about and there's going to be a lot more included usually and so you want to make sure that what you return is just the information that you care about giving back to the LLM like in this case we only care about sending back the actual data that's why I'm doing results data and not just result i could be lazy and just send in the entire result to the LLM but if that includes this massive JSON body from the API with a ton of extra information I risk overwhelming the LLM and just making it so it doesn't correctly pick out of that JSON the information that it actually cares about having and so that's why you want to format things and you'll see that as well with all the different tools that I set up um within the AI agent mastery course and then the very very last thing that I want to cover with you guys the last lesson the 20th one that I've got for you is in general what the anatomy of a good tool looks like and um this is kind of hard to see so I'm going to see if I can actually pull this up um within Windsurf here because I have I have this example live instead of just in a slide here so let me zoom in on this um because I want to just show you really quickly the anatomy of a good tool and so a lot of this is just covering what I've already showed um for the different tips that I have the different lessons for tools but I just want to quickly like tell you how this all applies in a single tool here so this is my full tool for rag you give some kind of user query it'll search the vector database and it'll return the relevant chunks for the agent to uh use that to aid in answering the user's question and so the first thing is we've got the examples in the prompt and I have a very detailed prompt here including different arguments that we have for this tool and then we have the whole thing wrapped in a try catch block so I've got the try and then the accept at the end when we get an error retrieving the documents from rag I also tell the agent exactly what that error is so the response includes the error message itself so it can reason about how to fix that problem when it would maybe invoke this tool a second time and then the other thing that I wanted to show you here uh for the anatomy of a good tool is what I talked about just a little bit ago where you want to only return the information that is relevant to the agent so in this case when we get this result from superbase and we're calling that match document function for rag we are getting all the documents and then we're just going to loop over them and create this string for each document that includes the ID the title the URL all that important metadata and then the content of that chunk as well we don't need anything else and trust me there is a lot more that we get back from this result object and so that's how we make sure that we're only giving what we actually care about the LLM receiving and then we're just returning that as a nicely formatted string because just like humans benefit from having things well formatted for them agents also benefit in the same way and that's often why oftentimes why you hear markdown as being the best format for agents is because markdown just formats things very nicely like let me open up a readme here so I can show you like this readme file like this is using markdown where we have sections and numbers and subheadings and um code blocks and things like that like all of that just makes it so that the agent has a better idea of how things are structured and so that's why also in my tools I'll spend the time I'll I'll actually code out creating a well formatted string to give back to the agent um but yeah what I'm going to do now is I'm going to enable Q&A and so I'll try to go through some questions in the regular chat as well um but yeah I I hope that those lessons made sense to you guys i hope that even if you're like an expert at building agents at least a few of those like "Oh yeah I didn't really think about it that way." I just hope that that can help you um build more effective agents and so yeah I just want to spend some time with some questions for you guys for 10 or 15 minutes here before we wrap it up um but yeah let me go through the chat and see if I can find anything and then also you guys can feel free to start posting in the uh Q&A channel as well yeah first congratulations on a booming community thank you very much i appreciate it uh and your question my tools are not forming properly there's no syntax error but still can't get my agent to start using tools what might be the problem yeah so I mean this depends a lot on what exactly you're building with like if it's N8N or Pantic AI so I'd be curious on that um because it yeah kind of sounds like the thing that I have to dive into a little bit more to really be able to to say um because there are a lot of things that could be wrong there um syntax there's no syntax error but I can't get my agent to start using the tools because it seems like there might just be like a connection error there between the agent and the tool um so I guess my first question is like will the agent ever use the tool or are you hitting a snag there where it doesn't even seem to recognize that the tool is there so I'd start with that hard to give like something super concrete um at that point um yeah let's see fred are there ways to lock in the system prompt so it does not scroll off as the context gets long perhaps encapsulating the system prompt information as memory so you kind of can i mean you could take the instructions from the system prompt and just like prepend the the latest user message with that like here are some instructions now here is the user message like you can send that in as the latest prompt to the LLM so you could definitely do that or you could kind of like you're saying have your general goals and preferences and and instructions as a part of the uh long-term memory so it could retrieve that and have that as a part of the latest user message as well so you definitely can um generally what I would say though is like if you're getting if you're hitting a problem where the system prompt is starting to get cut off you're going to want to resolve that anyway because if it gets through the system prompt and it loses the entire system prompt then you're going to start losing parts of the conversation as well so it's still going to be a problem like you don't really want to work around this issue and and like leave the issue as it is definitely want to address that and either use a different LLM that can handle longer context or figure out how you can make sure you don't hit that context limit like maybe for rag agents you'll just return less chunks like whatever that might be I would definitely address that as a priority instead of trying to work around it u but good question though yeah and then Ryan asked do you have a specific method to optimize system prompts especially when switching between LLMs um and so yeah I don't want to get into this a ton right now but this is where LLM eval EV valuations really come in handy um specifically LLM as a judge is a very powerful setup where you use another large language model to um usually evaluate the responses from your agent it can also generate the the requests to the agent as well and be used to automatically tune the system prompt so it can create you would kind of like give it your system prompt have it generate questions or requests to your agent evaluate the output and then automatically tweak the system prompt based on that um and usually I find that like manually adjusting the system prompt is more powerful but you can go down that route if you want but yeah usually it's either going to be like having some sort of automated evaluation and adjusting the prompt yourself based on those results or just having an LLM as a judge kind of like on either end automatically tweaking the system prompt yeah good question um next up oh I gotta scroll down whenever I click into the replies for a Q&A and I go back out I have to scroll back down which is kind of unfortunate um yeah I have a workflow where there is a WhatsApp chatbot using Puppeteer uh didn't provide an API key i use an API key that processes the payment system i only use the AI agent in the conversation part of the chatbot the rest I set with a classifier direct different actions such as creating a balance depositing withdrawing you think my approach is good um yeah i mean that sounds good to me um in in order for me to like really like give you a solid like yes or no I'd probably have to see it a bit more because this seems like quite an involved workflow um but I guess I'm curious like what exactly your thought is with the classifier there um I mean it sounds good but I yeah I definitely have to take a bit more of a closer look at that i mean and and honestly like you could you could build out both approaches because the tools will kind of be the same like those actions that you take either directed by the agent or just buy like a separate classifier like a um m like an ML setup there like you could definitely just test both um reuse those actions and see what works better um but yeah I mean that looks that's looks like a really cool use case by the way like that's awesome um Eric asked "Thank you for these lessons gold." I appreciate very much Eric uh where in the community can I refer back to it yeah good good question so all of the events that I have in Dynamus these work or these events and then like workshops that I do anything is going to be automatically recorded and then the recording will be uploaded within like around 30 minutes of the event being complete and the way that you can go back to that um actually let me share my screen quick so here in the homepage of Dynamus you can go to the events calendar you can click on any past event like I can go to the community introduction from yesterday and then click into it go to the page for this past event and then the recording will be right here for you to view um and so that'll be uploaded within 30 minutes of the event being complete so very very easy to go back through those events you can also go to the live events tab and then instead of um just viewing the upcoming events you can view past events and then you can also get into that event view um this way just like we did through the calendar so super easy to view those um those past events and get the recordings for those but yeah good question all right let's see we got a few more questions here and then I will wrap it up so it looks like we got four more so I'll try to get through these four more that we have in the Q&A uh and then I think I will call it there um yeah thank you Max by the way appreciate it you are very welcome thank you Andrea appreciate it guys i'm just taking a look at the uh regular chat quick here and then yeah let me go back to the Q&A all right scroll down patrick asked "With image recognition is there a way to use examples?" Examples here are the attributes from this image and use this as an example when classifying other images so typically for like an image recognition like if you're talking about like a machine learning algorithm or like a model that you're custom model that you're building up like a CNN or something the examples would be more like your input when you're training the model um so I I guess I'm not quite understanding your question unless you're talking about like using an LLM specifically with image analysis like not building your own image recognition model um I'm not sure if you mean that example here are the attributes from this image and use this as an example when classifying other images um because if you mean just like using LLM with vision capabilities you could definitely like include an image as a part of your prompt like you kind of do like like um twoot prompting where where you'd be like here's an image and then here's how I want you to classify it now here's another image how would you classify this like you could definitely do that just as it wouldn't be a part of your system prompt it would just be like a part of your user prompt doing something like a twoot prompting um yeah but good question though brian asks "Have you found that there is a limit to how many agents you can have under an orchestrator will it eventually get confused?" Yeah so there there certainly is a limit um because just like you can overwhelm an LLM with too many regular tools since each sub aent is essentially another tool you can definitely run into that issue where um there you might have like 20 different sub aents and that can start to overwhelm the LLM as far as how many it takes to overwhelm because it's usually pretty simple like here is your Slack agent here's your data analytics agent here's your database agent like it's usually pretty easy for the orchestrator to know when to call into each of them you can go pretty far like you can have a good number of sub aents under the orchestrator um like I've seen as many as like 15 to 20 and like it still works really well because usually that handoff is just really really simple um but like if you really want to get fancy you can start to have like an orchestrator that has sub orchestrators um I don't find that to be necessary like ever um but you definitely could do that if you just want to like kind of create this hierarchy of agents to like really make sure that you never have more than 10 tools for any given agent or whatever that might be um but yeah usually I don't find this to be an issue because if you have an agent that you actually want to orchestrate like 50 different sub aents you've probably got more of a problem there where you're just trying to do too much with one application and you might want to split that into different things in the first place as like entirely separate applications yeah good question all right next one is there a video in the community for vibe coding full stack app in Pantic AI any vibe coding for a serverless app yeah so as one of the videos in module four in the AI agent mastery course I talk about using AI coding assistance to help us build agents with Pyantic AI and I and I will say that I I don't generally condone vibe coding overall like I don't want to uh I don't want you to trust the LLM entirely when coding things i want you to be able to validate its output um but certainly using AI coding assistance is a big thing that I focus on for at least kind of getting that first 90% of the code created and then then you just perfecting it and fixing any errors that come up yourself and then as far as vibe coding um a full stack app that is something I'll cover more in module five of the AI agent mastery course when I build a full application around the agent that we built in the previous modules and then for serverless um I will be covering serverless for deploying agents in module six and so all that is coming soon i want to get these modules out for you guys as soon as possible um is there a support for calendarly one-on-one session with colon team for query features or client requests yeah good question so I'll be having um generally three office hour sessions a week which aren't going to be one-on-one sessions um because obviously there's just like so many different questions that all you guys have fantastic questions but I got to be able to address um you know a lot of people at the same time and so I I won't be offering one-on-one sessions but the office hours are going to be your place to come in with specific questions on your projects or like when you want to just get into the nitty-gritty details of things like that's what I'm going to be offering and I'm specifically offering many of those every single week just to make sure that there's so much time available for that that it's not going to be like 50 people coming into each office hour it's going to be a very small group of people and we can all learn together at the same time as we're going through these questions as well they can all be recorded sessions um yeah then the last question that we have here and then I'll close it off for all of us got one from Tom um let's see let me got to scroll back down all right so Tom asked "What are your thoughts about process the process of fine-tuning embedding models i feel like this is an important building block for accurate responses from rag maybe you could visit this one of your videos." Yeah absolutely great question fine-tuning embedding models is super super powerful and definitely something that I've been wanting to cover on my channel for a while the biggest problem is it's a bit more involved than a lot of the things I like typically cover on my channel so it's just a it's a hard video to create and it's it's not going to be as easy for people to like resonate with that really quick like oh yeah I'm going to like go use this in N right now because it's not that simple um but it is super super important and specifically I am going to be creating a course on knowledgebased agents like a rag specific course for Dynamis um sometime after the AI agent mastery course and fine-tuning embedding models is going to have a dedicated portion of that course for sure because it's super important um so yeah really good question and yeah really quick one from David if we've made changes to your repo how can we submit them you can make a poll request in any repo in the Dynamus organization if you want and so yeah right now the AI agent mastery repo is the only one that we have um so yeah definitely feel free to make a PR if you fix something up or just have a new feature i'll take a look and I'll be pretty picky about the things that I add into the repo but yeah I mean if there's like an issue that's like seriously needs to be addressed or something that I can add into the package without having to redo a bunch of videos or anything then yeah I'm all yours for that um so yeah good questions and um let me remove that there i'm going to go back to the chat uh Jiren said "Man such good content i appreciate it very much." Uh yeah Pan said "Vive coding is 90% code in the future so I believe having personalized coding agents should be a great add-on." Yeah definitely definitely um Aiden said "Appreciate the help and knowledge you give." Yeah thank you very much i'm glad I'm glad that this has been helpful for you guys um and yeah with that I'm generally going a little bit over in these events but I just love answering all your guys' questions and I appreciate them a ton and thank you all for being here and just hearing me out on all the lessons that I've learned at least a good chunk of lessons that I've learned from building agents over the years so I hope that was super helpful for you guys and of course if you have any questions on any of those lessons and you want me to elaborate on anything like please ask away in the community come to office hours next week with questions um I'd love to help address anything more that you guys are curious about and specifically like how you can apply these lessons into your projects i know that there's probably a lot of questions about that kind of stuff and so I'd love to get into the details of that with you as well so that is everything that I have for today thank you all for being here for coming with such great questions as well and I will see you guys around in the community
Sí, el texto anterior se interrumpió. Aquí está la continuación completa de la transcripción del video, a partir de donde se cortó:
...la misma conversación y esa es una de las razones por las que es realmente peligroso tener conversaciones largas, especialmente en cosas como tus asistentes de codificación de IA, porque si entendió algo incorrectamente en tu base de código o en la conversación anterior, podría cometer el mismo error de nuevo. He visto esto antes donde, por ejemplo, hubo un error al invocar una herramienta como la herramienta de búsqueda web, y luego, más tarde en la conversación, le pides que haga la búsqueda web de nuevo y dirá que no puede, que ni siquiera lo intentará, porque simplemente sabe por el punto anterior de la conversación que eso no funcionó, incluso si le dijiste más tarde que, oh sí, hiciste esto mal, por eso no funcionó. Así que sí, esa es una gran razón por la que querrás iniciar nuevas conversaciones con bastante frecuencia con tus agentes y asistentes de codificación de IA.
Y lo siguiente, tengo el prototipo del agente N8N que construimos en el curso AI Agent Mastery, porque lo más importante que me ayudó a entender mejor la memoria a largo plazo —y tal vez esto es obvio para muchos de ustedes, pero esto realmente me caló hondo— es que la memoria a largo plazo para tus agentes es solo otro RAG (Retrieval-Augmented Generation), eso es literalmente todo lo que es. Así, dentro de nuestro prototipo, tenemos la herramienta para que nuestro agente recupere memorias y luego tenemos la herramienta para que recupere documentos, ambas utilizando solo RAG. Puedes ver que la herramienta es exactamente la misma. Y la forma en que insertamos memorias en Supabase es usando el mismo nodo en N8N que usamos para insertar nuestros documentos en Supabase dentro de nuestra tubería RAG. La razón por la que esto es tan importante es porque muchas de estas estrategias que aprendemos para una mejor recuperación RAG, como la expansión de consultas o la reclasificación, todas estas estrategias detalladas se aplican también al trabajar con la memoria a largo plazo, porque es esencialmente otra tabla en tu base de datos vectorial, solo que con memorias generadas por el agente en lugar de documentos que estás extrayendo de tu fuente de datos. Obviamente, no todas las estrategias se aplicarán a las memorias y tienes que tratarlas de manera diferente a tus documentos, pero en general, muchas de esas formas de hacer que RAG sea más preciso también se aplicarán a las memorias, así que es muy bueno saber eso.
Y luego, la última lección sobre memoria que tengo es que querrás incluir las llamadas a herramientas en tu historial de conversaciones. Por lo general, he visto a mucha gente hacer esto —y de hecho, es realmente desafortunado que los agentes N8N también lo hagan— donde no incluyen en el historial de conversaciones nada relacionado con las herramientas. Lo que queremos hacer es incluir todo lo relacionado con las herramientas también. Así, cada vez que el agente hace una solicitud para usar una herramienta, como enviar un correo electrónico o leer un mensaje de Slack, queremos incluir esa solicitud y luego la respuesta que obtenemos al llamar a esta herramienta. La razón de esto es que muchas veces el agente puede volver a referenciar cosas que fueron devueltas por las llamadas a herramientas en una solicitud posterior que tenemos para él en la conversación. Un buen ejemplo de esto es RAG. Cuando realizas una búsqueda en tu base de datos vectorial y recuperas fragmentos relevantes, el agente usará eso para luego responder al mensaje del usuario. Pero luego el usuario podría tener un mensaje de seguimiento que también podría ser respondido a partir de los fragmentos que recuperó de esa primera búsqueda RAG. Y así, si tenemos las respuestas de las herramientas como parte del historial de la conversación, el agente puede volver a referenciar eso para responder a la segunda pregunta del usuario sin tener que hacer otra búsqueda. Es más rápido y hace que sea más probable que el agente dé una respuesta correcta.
Y luego, la última cosa que quiero cubrir aquí es el enfoque en las herramientas. Quiero sumergirme en la anatomía de una buena herramienta, cómo puedes configurar una herramienta y todos los diferentes componentes. La primera cosa es que las descripciones que das a tus herramientas son clave. La descripción de la herramienta es tu lugar para decirle al agente cómo usar la herramienta individual. Y luego, el system prompt (indicación del sistema) es donde puedes decirle al agente cómo usar diferentes herramientas juntas.
Para herramientas más complejas, queremos dar ejemplos específicamente ejemplos de cómo podrían verse los parámetros. Y luego, para casi todas tus herramientas, querrás capturar los errores y luego devolver el problema al agente. Siempre envuelvo todo en un bloque try y luego tengo un except. La razón por la que quieres hacer esto es doble: la primera es que si un agente usa una herramienta incorrectamente, no quieres que tu aplicación falle. La otra cosa es que cuando le dices al agente exactamente qué salió mal, puedes darle la oportunidad de quizás invocar la herramienta una segunda vez, corrigiendo los parámetros.
Lo siguiente para tus herramientas es que quieres ser muy cuidadoso de devolver solo lo que el LLM necesita saber. No quieres abrumar al LLM con un cuerpo JSON masivo de la API que incluya un montón de información extra. Querrás formatear las cosas y verás eso con todas las diferentes herramientas que configuro.
La última lección (la número 20) es la anatomía de una buena herramienta. Un ejemplo de una herramienta RAG completa incluye ejemplos en la indicación (prompt), tiene todo envuelto en un bloque try-catch, y solo devuelve la información que es relevante para el agente. Devuelvo esa información como una cadena bien formateada, porque al igual que los humanos se benefician de tener las cosas bien formateadas, los agentes también se benefician de la misma manera. Por eso Markdown se considera a menudo el mejor formato para los agentes.
Posteriormente, el presentador abre un tiempo de preguntas y respuestas (Q&A).
Resumen de Q&A (Preguntas y Respuestas):
	•	Herramientas que no funcionan: Si el agente no usa las herramientas, puede deberse a un error de conexión entre el agente y la herramienta, o a que el agente no reconoce la herramienta.
	•	Gestión del System Prompt (Indicación del Sistema) largo: Si el system prompt se está cortando debido a la longitud del contexto (context length), es mejor abordar el problema directamente usando un LLM diferente que maneje un contexto más largo, o ajustando estrategias (como devolver menos fragmentos RAG) en lugar de tratar de evitar el problema. Si se pierde el system prompt, también se perderán partes de la conversación.
	•	Optimización del System Prompt al cambiar de LLM: Las evaluaciones de LLM son muy útiles, especialmente el uso de un LLM como juez (LLM as a judge). Esto permite usar otro LLM para evaluar las respuestas del agente, generar solicitudes y ajustar automáticamente la indicación del sistema.
	•	Límite de agentes bajo un orquestador: Ciertamente hay un límite, ya que cada subagente es esencialmente otra herramienta. Si se sobrecarga un LLM, puede confundirse. Sin embargo, la transferencia de tareas suele ser simple, por lo que se puede tener un buen número de subagentes (se han visto hasta 15 a 20). Si se necesita orquestar 50 subagentes, podría ser mejor dividir la aplicación en diferentes aplicaciones separadas.
	•	Vibe Coding (Codificación intuitiva): El presentador cubre el uso de asistentes de codificación de IA para construir agentes con Pyantic AI en el módulo cuatro del curso AI Agent Mastery. También cubrirá el desarrollo de una aplicación completa en el módulo cinco y la implementación serverless de agentes en el módulo seis. El presentador no recomienda confiar completamente en el LLM (vibe coding) sino usarlo para generar el 90% inicial del código y luego perfeccionarlo manualmente.
	•	Soporte y sesiones uno a uno: No se ofrecerán sesiones uno a uno, pero sí tres sesiones de office hours a la semana en la comunidad para preguntas específicas sobre proyectos.
	•	Ajuste fino de modelos de embedding (Fine-tuning embedding models): Se considera que el ajuste fino de modelos de embedding es superpoderoso e importante para respuestas RAG precisas. Este tema tendrá una porción dedicada en un curso RAG específico para Dynamis que se creará después del curso AI Agent Mastery.
	•	Envío de cambios a repositorios: Los cambios se pueden enviar haciendo un pull request (PR) en cualquier repositorio de la organización Dynamus (actualmente solo el repositorio AI agent mastery).
This document integrates the full transcript content into a structured RAG-optimized format, providing a comprehensive overview of the top 20 lessons learned from building hundreds of AI agents. The content is presented entirely in English, as requested.

I. AI Agent Definition and Core Components
These lessons stem from a special live event hosted during the launch week of the Dynamus community, celebrating the Archon launch and reaching a thousand members. The goal is to share 20 practical lessons learned over the last couple of years.
A. What is an AI Agent? (Definition)
An AI agent is defined as a program that uses a large language model to reason about how it interacts with its environment and to take varying courses of actions to achieve a goal.
	1	Interacting with the Environment: This involves tools that allow the agent to perform actions, such as pulling messages from a Slack conversation or drafting an email in Gmail or Outlook.
	2	Varying Courses of Action (Non-Determinism): The agent can decide how many steps it wants to take (five, three, or one) based on the goal. This makes agents both powerful and dangerous because this behavior is non-deterministic.
B. The Four Components of an AI Agent
This structure is covered in Module 1 of the AI Agent Mastery Course:
	1	Agent Program: This is the system prompt, containing general instructions for the agent's behavior and the desired tone.
	2	Large Language Model (LLM): The "brain" of the agent, possessing all the reasoning capability.
	3	Tools: The capabilities provided to the agent to interact with its environment.
	4	Memory System: Includes short-term memory (like conversation history) and long-term memory, which enables the agent to remember preferences and goals between conversations, and encompasses RAG (Retrieval-Augmented Generation) and knowledge bases.

II. High-Level Lessons: Addressing Hallucinations and Non-Determinism
The biggest challenge with agents is the idea of hallucinations. When AI agents hallucinate (make a mistake or make up information), they act so confident when presenting the wrong data, sometimes even citing sources that do not exist with 100% certainty. This problem stems from the concept of non-determinism in software development.
Lesson
Focus & Explanation
Citation
Lesson 1
Use AI to Save Time, Not Replace You Entirely. Traditional software is deterministic (same input always gives the same output). With non-determinism, an agent might work fine the first 50 times but "completely botch it" the 51st time with the exact same request. To mitigate this danger, share the responsibility with the AI. Example: Have the agent automatically create email drafts (saving time) rather than automatically replying to every email (replacing you entirely). This way, correcting a hallucination only costs a bit of time.

Lesson 2
Do Not Skimp on Planning and Prototyping. Dedicate time upfront to building a good foundation. The presenter dedicates two entire modules in the course to this phase. Spending five hours planning can save 20 hours of development later, avoiding the need to scrap all implementation work.

Lesson 3
Beware the Hallucination Explosion (Compounding Non-Determinism). In a multi-agent workflow, the inaccuracies of multiple LLMs build on top of each other. If three agents each work successfully 95% of the time, the entire system only works successfully 86% of the time (0.95 x 0.95 x 0.95). The more agents involved, the more likely these errors are to compound.

Lesson 4
Implement AI Agent Guardrails. Guardrails are pieces of logic that run before or after calling the LLM/agent. If a potential hallucination source is detected (input) or an actual hallucination is detected (output), the system follows a failure path. Example: An input guardrail using a lightweight LLM checks if a user's travel budget (e.g., $500 for Dubai) is reasonable to prevent forcing the agent into a hallucination. Example: An output guardrail verifies the itinerary covers the correct number of days requested.

Lesson 5
Leverage Specialized Agents. Distribute responsibility among many agents that specialize in different things (e.g., one agent for Slack tool calls, another for database tool calls). This leads to much better results by avoiding overwhelming a single LLM with too much responsibility. The primary orchestrator agent rarely picks the wrong specialized agent, especially if the roles are distinct and clearly defined in the system prompt.

Lesson 6
Examples, Examples, and Examples. Providing concrete examples in the system prompt is extremely helpful to the LLM. This practice is common in the best AI coding assistants like Vzero, Cursor, and Bolt new. Examples should include all nitty-gritty details (like pluses, minuses, line numbers for diffs) and clarify how the agent should use tools or format output.


III. Component-Specific Lessons
C. Agent Program (System Prompt) Lessons
Lesson
Focus & Explanation
Citation
Lesson 7
Avoid Adding "Negatives". This is crucial for longer system prompts. LLMs "love to drop the negative" (ignoring "do not") and perform the very thing they were told not to do. Solution: Tell the agent what to do instead of what not to do; e.g., use "use fifth grade level English" instead of "do not use complex language".

Lesson 8
Avoid Contradictions. Contradictions cause the LLM to randomly choose between conflicting instructions, leading to very inconsistent results and hallucinations. Example: Instructing a "knowledge assistant" to be concise but also requiring comprehensive coverage including historical context and theoretical frameworks in the subsequent paragraph is a contradiction. Another example involved an agent told to be accommodating to the user but simultaneously rigid when applying calendar tool instructions, resulting in the agent offering unavailable times.

Lesson 9
Version Your Prompts. Just like versioning code, prompt versioning is essential. This allows developers to quickly revert to a previous version if a new version fails (e.g., becomes too long, overwhelms the LLM, or contains an accidental contradiction). Tools like Langfuse are recommended for prompt management.

D. Large Language Model (LLM) Lessons
Lesson
Focus & Explanation
Citation
Lesson 10
Swapping LLMs Can Be Dangerous. Even when swapping to an LLM supposed to be better (e.g., GPT-4o over GPT-4 Turbo), agents can exhibit "super weird hallucinations". Different LLMs understand system prompts and take actions differently. Developers must perform a lot of testing and often significantly tweak the system prompt when switching.

Lesson 11
Your Favorite LLM Isn't Always the Best. Be open-minded and willing to test different LLMs, as they perform better for different things. Examples: Claude 3.7 Sonnet is often found best for coding, while Gemini 2.5 Pro works better for creative agents (like writing assistants).

Lesson 12
Watch Your Context Lengths. This is most applicable to local LLMs, which often have smaller limits (e.g., 32,000 tokens). Since conversation history is part of the prompt, a long conversation can hit the limit. The system prompt (high-level instructions) is the first thing to be lost because it sits at the very top of the conversation context. Losing the system prompt causes the agent to completely forget instructions and tool access (e.g., stops calling the web search tool).

E. Memory Systems Lessons (Short-Term & Long-Term)
Lesson
Focus & Explanation
Citation
Lesson 13
Previous Hallucinations Are Likely to be Repeated. Even after a user corrects a factual error in a conversation (e.g., correcting a publication date), the LLM may repeat the original hallucination later in the same conversation. This makes long conversations dangerous. Action: Start new conversations quite often.

Lesson 14
Long-Term Memory is Just Another RAG. Long-term memory mechanisms are essentially the same as document retrieval (RAG). The prototype for the N8N agent confirms that tools to retrieve memories and documents are the exact same RAG mechanism. Therefore, RAG strategies like query expansion or re-ranking are applicable to working with long-term memory.

Lesson 15
Include Tool Calls in Your Conversation History. Conversation history must include the agent's request to use a tool (e.g., send an email) and the response/success obtained from that tool call. This is critical because the agent can re-reference successful tool responses (like retrieved RAG chunks) to answer a subsequent follow-up message without needing another lookup, making the agent faster and more accurate.

F. Tool Lessons (The Anatomy of a Good Tool)
Lesson
Focus & Explanation
Citation
Lesson 16
Tool Descriptions Are Key. The tool description is included in the LLM prompt to inform the agent when and how to use a tool. The general rule is: the tool description covers the nitty-gritty details of the individual tool; the system prompt covers how to use different tools together to achieve a single goal.

Lesson 17
Give Parameter Examples for Complex Tools. For tools with complex parameters (like a RAG retrieval query), examples of how to format the parameters are extremely helpful.

Lesson 18
Catch Errors and Return the Problem to the Agent. Every tool should be wrapped in a try block and an except block. This prevents the application from crashing if the agent hallucinates a bad parameter. The tool must return the error message to the agent so it can reason about the problem and attempt to reinvoke the tool correctly.

Lesson 19
Only Return What the LLM Needs to Know. Avoid sending massive JSON bodies or unnecessary metadata (e.g., query time from APIs like Superbase or Shopify) back to the LLM. This risks overwhelming the LLM and prevents it from correctly picking out the relevant data. The output should be formatted (e.g., results.data instead of the entire result).

Lesson 20
Format the Output Nicely (Markdown). Just as humans benefit from well-formatted information, agents benefit too. The best format for agents is often Markdown, as it structures things clearly with sections, subheadings, and code blocks, improving the agent's understanding.


IV. Supplementary Course and Q&A Details
The lessons concluded with a Q&A session addressing common development concerns.
A. Development Practices and Optimization
	•	Prompt Management: Tools like Langfuse are considered highly effective for managing and versioning system prompts.
	•	Optimizing Prompts Between LLMs: A specific method is using LLM as a judge, where a second LLM evaluates agent responses and automatically tweaks the system prompt to improve performance with the new model.
	•	Context Length Workarounds: If the system prompt is being cut off due to context limits, the primary solution should be to address the issue directly (e.g., use a different LLM or return fewer RAG chunks) rather than trying to prepend instructions or encapsulate the system prompt information as memory.
B. Multi-Agent Systems and Architecture
	•	Orchestrator Limits: An orchestrator agent can handle a "good number" of sub-agents—up to 15 to 20—because the task handoff is typically simple. If an agent requires orchestrating 50 different sub-agents, it is likely that the application is trying to do too much and should be split into entirely separate applications.
C. Course Modules and Community Resources
	•	"Vibe Coding": The presenter does not generally condone trusting the LLM entirely, but focuses on using AI coding assistance to generate the first 90% of the code, which the developer then perfects. Full-stack application development is covered in Module 5, and serverless deployment for agents is covered in Module 6 of the AI Agent Mastery Course.
	•	Community Support: The community offers three office hour sessions a week for specific questions on projects and nitty-gritty details, though one-on-one sessions are not offered.
	•	Fine-Tuning Embedding Models: This is considered super powerful and important for accurate RAG. A dedicated course on knowledge-based agents (RAG-specific) will cover this topic after the AI Agent Mastery Course.
	•	Submitting Changes: Changes to the Dynamus repository (e.g., the AI agent mastery repo) can be submitted via a pull request (PR).
