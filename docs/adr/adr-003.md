# ADR-003: Validación del Output - RAGAS como Gate de Calidad

**Estado:** Propuesto  
**Fecha:** 2025-01-27  
**Contexto:** Pipeline RAG - Fase 3  
**Depende de:** ADR-002 (Calidad del Input)

## Decisión

Integrar RAGAS (Retrieval-Augmented Generation Assessment) como sistema de evaluación automática y gate de calidad para outputs del sistema RAG, integrado con TaskDB.

## Contexto

### Problema Actual
- Ausencia de evaluación sistemática de calidad de outputs RAG
- Imposibilidad de auditar mejoras en el pipeline
- Riesgo de degradación silenciosa de calidad
- Falta de métricas objetivas para decisiones técnicas

### Requerimientos
- Evaluación automática de calidad de respuestas RAG
- KPIs cuantificables: fidelidad, relevancia, recall
- Integración con TaskDB para trazabilidad
- Gate obligatorio para PRPs críticos

## Opciones Consideradas

### Opción A: Evaluación manual
**Pros:**
- Control total sobre criterios de evaluación
- Flexibilidad en casos específicos

**Contras:**
- No escalable
- Subjetividad inherente
- Alto costo operacional
- Imposible automatizar

### Opción B: RAGAS (Elegida)
**Pros:**
- Framework especializado en evaluación RAG
- Métricas estandarizadas y validadas
- Integración con ecosistema Python
- Automatización completa

**Contras:**
- Complejidad de configuración inicial
- Dependencia externa

### Opción C: Métricas custom
**Pros:**
- Optimización específica para casos de uso
- Control total sobre implementación

**Contras:**
- Alto costo de desarrollo
- Falta de validación externa
- Mantenimiento complejo

## Decisión

**Se adopta la Opción B: Integración de RAGAS**

### Justificación
- Transforma calidad de respuesta en métrica auditable
- Framework especializado y validado por la comunidad
- Capacidad de automatización completa
- Integración natural con TaskDB existente

## Implementación

### Componentes
1. **RAGAS Evaluator**: Sistema de evaluación automática
2. **TaskDB Integration**: Métricas `taskdb_rag_eval_score`
3. **Quality Gates**: Bloqueo de PRPs críticos sin evaluación
4. **Dashboard**: Visualización de métricas de calidad

### KPIs Definidos
- **Fidelidad**: Precisión de información extraída
- **Relevancia**: Pertinencia de respuestas al contexto
- **Recall**: Cobertura de información relevante
- **Precisión**: Exactitud de información proporcionada

### Archivos Afectados
- `eval/ragas_smoke.py` (evaluador principal)
- `core/taskdb/` (integración con TaskDB)
- `config/rag.yaml` (configuración de evaluación)
- `scripts/ci-rag-eval.sh` (gate de CI/CD)

### Métricas de Éxito
- Evaluación automática de 100% de outputs críticos
- Tiempo de evaluación < 30 segundos por query
- Cobertura de KPIs > 95%

## Consecuencias

### Positivas
- ✅ Calidad de outputs cuantificable y auditable
- ✅ Detección temprana de degradación
- ✅ Base objetiva para decisiones técnicas
- ✅ Integración con pipeline CI/CD

### Negativas
- ⚠️ Complejidad adicional en pipeline
- ⚠️ Overhead computacional en evaluación
- ⚠️ Dependencia externa (RAGAS)

### Riesgos
- **Alto**: Falsos positivos en evaluación
- **Medio**: Degradación de rendimiento
- **Bajo**: Incompatibilidad con casos específicos

## Configuración de Gates

### PRPs Críticos
- Documentación técnica
- Procedimientos de seguridad
- Políticas de compliance
- Decisiones arquitecturales

### Umbrales de Calidad
- **Fidelidad**: ≥ 0.85
- **Relevancia**: ≥ 0.80
- **Recall**: ≥ 0.75
- **Precisión**: ≥ 0.85

### Acciones en Caso de Fallo
1. Bloqueo automático del PRP
2. Notificación al equipo responsable
3. Análisis de causa raíz
4. Reevaluación post-corrección

## Seguimiento

### Criterios de Éxito
- [ ] RAGAS integrado y funcionando
- [ ] Gates de calidad activos para PRPs críticos
- [ ] Métricas disponibles en TaskDB
- [ ] Dashboard de calidad operativo

### Revisión
- **Fecha:** 30 días post-implementación
- **Responsable:** Equipo RAG + QA
- **Criterios:** Cumplimiento de métricas de éxito y efectividad de gates

---

**Próximo ADR:** ADR-004 - PRPs como Código con DSPy
